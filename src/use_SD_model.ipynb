{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "650e1224-d757-45e9-a7cc-67d608bd73bd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "from huggingface_hub import notebook_login\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import logging\n",
    "import os\n",
    "from huggingface_hub import HfFolder\n",
    "import random\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import time\n",
    "\n",
    "fixed_seed = 1\n",
    "torch.manual_seed(fixed_seed)\n",
    "np.random.seed(fixed_seed)\n",
    "# Check if the HUGGING_FACE_TOKEN environment variable is set\n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "if token:\n",
    "    # If the token is found in the environment, use it to authenticate\n",
    "    # This saves the token for use in the current session\n",
    "    HfFolder.save_token(token)\n",
    "else:\n",
    "    # If the token is not found, prompt for manual login\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda:0\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "if \"mps\" == torch_device:\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "dtype = torch.float32\n",
    "print(f\"Using device: {torch_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0f6f2476877e4160b88fced6f952df2d",
      "0fe6a1d30e644347884c29fc571429b8",
      "525a3d7e3f984912a0b7189bdf0c4b9b",
      "8d255326a1cc4debb7ea4691c5d81965",
      "935ef00936d843bebcce486695fd9a8d",
      "5bc66cf9963748d693e44280f95f38e7",
      "f51404a8ffd34b9a84512cac5515cfbc",
      "efbb6d92cb1049819ff5ee1902ef42cc",
      "fb24169803a740ce93bc664be4c290a4",
      "721cf3f0f4784a23a35ad1077c7624c5",
      "d2d4b291be2844a3a18bf6401c7d0ee0"
     ]
    },
    "id": "eb58f35f-dde0-428f-92d8-606fd3c0676d",
    "outputId": "ea7f17da-9ff3-4648-cef2-34bb627524ce"
   },
   "outputs": [],
   "source": [
    "model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "\n",
    "SD_path = \"../../OCT-Longitudinal/saved_models/SD_OCT\"\n",
    "unet = UNet2DConditionModel.from_pretrained(SD_path, subfolder=\"unet\")\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "\n",
    "vae = vae.to(torch_device, dtype=dtype)\n",
    "unet = unet.to(torch_device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dim(caption, target_shape=(77, 768)):\n",
    "    caption = np.array(caption)\n",
    "    final_array = np.zeros(target_shape)\n",
    "    # Determine the length of the caption to be inserted\n",
    "    assert target_shape[1] >= len(caption)\n",
    "    caption_len = len(caption)\n",
    "    # Insert the caption into the beginning of the final_array\n",
    "    for i in range(target_shape[0]):\n",
    "        final_array[i, 0:caption_len] = caption\n",
    "    return final_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an image from a random latent space\n",
    "\n",
    "prompt = np.random.randn(128)\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "    real_input = prompt\n",
    "    real_input_mat = convert_dim(real_input)\n",
    "    real_input_mat = torch.tensor(real_input_mat)\n",
    "    real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "    input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "    for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    latents = latents.to(dtype=dtype)\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "# Display\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image).convert(\"L\")\n",
    "              for image in images]  # convert(\"L\")->to gray scale\n",
    "image = pil_images[0]\n",
    "image = image.convert(\"L\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the latent vector from an image in the test set to generate an image with the diffusion model and compare it with the original image.\n",
    "\n",
    "# Path to the directories\n",
    "image_dir = \"../../OCT-Longitudinal/test_images\"\n",
    "json_dir = \"../../OCT-Longitudinal/128_dim_latent_space/test_set\"\n",
    "\n",
    "# List all JPEG files in the directory\n",
    "jpeg_files = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
    "# Randomly select one JPEG file\n",
    "selected_jpeg = random.choice(jpeg_files)\n",
    "# Identify the corresponding JSON file\n",
    "json_filename = selected_jpeg.replace('.jpeg', '.json')\n",
    "json_path = os.path.join(json_dir, json_filename)\n",
    "\n",
    "# Read the vector from the JSON file\n",
    "print(f\"Reading the vector from the JSON file: {json_path}\")\n",
    "with open(json_path, 'r') as file:\n",
    "    prompt = json.load(file)\n",
    "\n",
    "# Load and display the original image\n",
    "original_image_path = os.path.join(image_dir, selected_jpeg)\n",
    "original_image = Image.open(original_image_path)\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "    real_input = prompt\n",
    "    real_input_mat = convert_dim(real_input)\n",
    "    real_input_mat = torch.tensor(real_input_mat)\n",
    "    real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "    input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "    for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample  #NOTE: not doing classifier free here!\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    latents = latents.to(dtype=dtype)\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "# Display\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "image = pil_images[0]\n",
    "image = image.convert(\"L\")\n",
    "# concatenate the images\n",
    "concatenated = Image.new('L', (2 * width, height))\n",
    "# resize original image to with and height\n",
    "original_image = original_image.resize((width, height))\n",
    "concatenated.paste(original_image, (0, 0))\n",
    "concatenated.paste(image, (width, 0))\n",
    "display(concatenated)\n",
    "save_dir = \"data/generate_images/diffusion_compare_rel_synth\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Find the highest numbered file in the directory\n",
    "existing_files = [f for f in os.listdir(save_dir) if f.endswith('.jpeg')]\n",
    "# Filter out files that do not start with a digit before finding the highest number\n",
    "existing_files_filtered = [f for f in existing_files if f.split('.')[0].isdigit()]\n",
    "if existing_files_filtered:\n",
    "    highest_num = max([int(f.split('.')[0]) for f in existing_files_filtered])\n",
    "else:\n",
    "    highest_num = 0\n",
    "\n",
    "# File name for the new concatenated image\n",
    "new_file_name = f\"{highest_num + 1}.jpeg\"\n",
    "save_path = os.path.join(save_dir, new_file_name)\n",
    "\n",
    "# Save the concatenated image\n",
    "concatenated.save(save_path)\n",
    "print(f\"Concatenated image saved as: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image grid\n",
    "x = 0\n",
    "base_dir = f\"../data/generate_images/diffusion_images/{x}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "selected_features = np.array(   [92,111,50,3,91,67,37,8,90,120,54,56,21,61,75,29,80,12,95,118,73,94,101,20,48,99,104,13,59,52,106,79,4,86,93,85,72,32,87,35,47,113,40,53,36,55,122,22,5,2,88,77,26,15,7,108,58,28,39,128,126,25,103,65,105,34,18,69,27,43,64,123,38,78,17,121,42,49,33,66,57,6,24,112,10,115,68,45,11,51,41,97,70,102,114,89,71,44,110,109,62,31,124,16,1,74,9,119,14,83,117,76,60,46,23,84,98,82,100,107,81,125,127,30,19,96,63,116]\n",
    ")\n",
    "# subtract 1 from all elements in selected_features (julia -> python)\n",
    "selected_features -= 1\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents_orig = latents * scheduler.init_noise_sigma\n",
    "\n",
    "save_names = [\"verticle_position\", \"tilt\", \"luminance\", \"curvature\"]\n",
    "cols = 7\n",
    "rows = len(save_names)\n",
    "max_val = 4\n",
    "for j in range(rows):\n",
    "    save_name = save_names[j]\n",
    "    for i in range(cols):\n",
    "        prompt = np.zeros(128)\n",
    "        prompt[selected_features[0]] = 3.0\n",
    "        prompt[selected_features[j + 1]] = -max_val + i * 2 * max_val / (cols -1)\n",
    "        latents = latents_orig.clone()\n",
    "\n",
    "        # Loop\n",
    "        with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "            real_input = prompt\n",
    "            real_input_mat = convert_dim(real_input)\n",
    "            real_input_mat = torch.tensor(real_input_mat)\n",
    "            real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "            input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "            for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "                latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        # scale and decode the image latents with vae\n",
    "        latents = 1 / vae.config.scaling_factor * latents\n",
    "        with torch.no_grad():\n",
    "            latents = latents.to(dtype=dtype)\n",
    "            image = vae.decode(latents).sample\n",
    "\n",
    "        # Display\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        image = pil_images[0]\n",
    "        # to Gray scale\n",
    "        image = image.convert(\"L\")\n",
    "        # display(image)\n",
    "        image_path = f\"{base_dir}/{save_name}_{i}.png\"\n",
    "        image.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nr = 0\n",
    "base_dir = f\"../data/generate_images/diffusion_images\"\n",
    "\n",
    "# Define the names used for saving the images\n",
    "save_names = [\"verticle_position\", \"tilt\", \"luminance\", \"curvature\"]\n",
    "\n",
    "# Initialize a list to hold the loaded images\n",
    "loaded_images = []\n",
    "\n",
    "# Load the images\n",
    "for j, save_name in enumerate(save_names):\n",
    "    for i in range(cols):\n",
    "        image_path = f\"{base_dir}/{img_nr}/{save_name}_{i}.png\"\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                loaded_images.append(img.copy())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "# Assuming all images are of the same size\n",
    "img_width, img_height = loaded_images[0].size\n",
    "\n",
    "# Create a new image with a size to hold all the images in a 4x7 grid\n",
    "grid_image = Image.new(\"L\", (img_width * cols, img_height * len(save_names)))\n",
    "\n",
    "# Paste the images into the grid\n",
    "for j, img in enumerate(loaded_images):\n",
    "    x = j % cols * img_width\n",
    "    y = j // cols * img_height\n",
    "    grid_image.paste(img, (x, y))\n",
    "\n",
    "# Display the concatenated image\n",
    "display(grid_image)\n",
    "\n",
    "# Optionally, save the grid image\n",
    "grid_image_path = f\"{base_dir}/concatenated_grid_{img_nr}.png\"\n",
    "grid_image.save(grid_image_path)\n",
    "print(f\"Concatenated grid image saved at: {grid_image_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "diff_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
