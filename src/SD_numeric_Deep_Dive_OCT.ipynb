{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "id": "650e1224-d757-45e9-a7cc-67d608bd73bd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from base64 import b64encode\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel, PNDMScheduler, DDPMScheduler\n",
    "from huggingface_hub import notebook_login\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import logging\n",
    "import os\n",
    "from huggingface_hub import HfFolder\n",
    "import random\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "\n",
    "fixed_seed = 1\n",
    "torch.manual_seed(fixed_seed)\n",
    "np.random.seed(fixed_seed)\n",
    "# Check if the HUGGING_FACE_TOKEN environment variable is set\n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "if token:\n",
    "    # If the token is found in the environment, use it to authenticate\n",
    "    # This saves the token for use in the current session\n",
    "    HfFolder.save_token(token)\n",
    "else:\n",
    "    # If the token is not found, prompt for manual login\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda:0\" if torch.cuda.is_available(\n",
    ") else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "if \"mps\" == torch_device:\n",
    "    os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "print(f\"Using device: {torch_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0f6f2476877e4160b88fced6f952df2d",
      "0fe6a1d30e644347884c29fc571429b8",
      "525a3d7e3f984912a0b7189bdf0c4b9b",
      "8d255326a1cc4debb7ea4691c5d81965",
      "935ef00936d843bebcce486695fd9a8d",
      "5bc66cf9963748d693e44280f95f38e7",
      "f51404a8ffd34b9a84512cac5515cfbc",
      "efbb6d92cb1049819ff5ee1902ef42cc",
      "fb24169803a740ce93bc664be4c290a4",
      "721cf3f0f4784a23a35ad1077c7624c5",
      "d2d4b291be2844a3a18bf6401c7d0ee0"
     ]
    },
    "id": "eb58f35f-dde0-428f-92d8-606fd3c0676d",
    "outputId": "ea7f17da-9ff3-4648-cef2-34bb627524ce"
   },
   "outputs": [],
   "source": [
    "model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\")\n",
    "\n",
    "# unet_ema_path = \"../data/300000-step-model-OCT/checkpoint-116000/\" # TODO change back to this\n",
    "unet_ema_path = \"/store/CIA/scfc3/diffusion/bloodimage/saved_models/500000-step-model-pumas-VAE/checkpoint-21000\"\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    unet_ema_path, subfolder=\"unet_ema\")\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "\n",
    "vae = vae.to(torch_device, dtype=dtype)\n",
    "unet = unet.to(torch_device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dim(caption, target_shape=(77, 768)):\n",
    "    caption = np.array(caption)\n",
    "    final_array = np.zeros(target_shape)\n",
    "    # Determine the length of the caption to be inserted\n",
    "    assert target_shape[1] >= len(caption)\n",
    "    caption_len = len(caption)\n",
    "    # Insert the caption into the beginning of the final_array\n",
    "    for i in range(target_shape[0]):\n",
    "        final_array[i, 0:caption_len] = caption\n",
    "    return final_array\n",
    "\n",
    "\n",
    "selected_features = np.array([92, 111, 50, 3, 91, 67, 37, 8, 90, 120, 54, 56, 21, 61, 75, 29, 80, 12, 95, 118, 73, 94, 101, 20, 48, 99, 104, 13, 59, 52, 106, 79, 4, 86, 93, 85, 72, 32, 87, 35, 47, 113, 40, 53, 36, 55, 122, 22, 5, 2, 88, 77, 26, 15, 7, 108, 58, 28, 39, 128, 126, 25,\n",
    "                             103, 65, 105, 34, 18, 69, 27, 43, 64, 123, 38, 78, 17, 121, 42, 49, 33, 66, 57, 6, 24, 112, 10, 115, 68, 45, 11, 51, 41, 97, 70, 102, 114, 89, 71, 44, 110, 109, 62, 31, 124, 16, 1, 74, 9, 119, 14, 83, 117, 76, 60, 46, 23, 84, 98, 82, 100, 107, 81, 125, 127, 30, 19, 96, 63, 116])\n",
    "# subtract 1 from all elements in selected_features (julia -> python)\n",
    "selected_features -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = \"../data/generate_images/generate_json_latent\"\n",
    "files = os.listdir(prompt_path)  # Filter out only .json files\n",
    "json_files = [file for file in files if file.endswith(\n",
    "    '.json')]  # Pick a random .json file\n",
    "random_json_file = random.choice(json_files)\n",
    "full_path = os.path.join(prompt_path, random_json_file)\n",
    "with open(full_path, 'r') as f:\n",
    "    prompt = json.load(f)\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "    real_input = prompt\n",
    "    real_input_mat = convert_dim(real_input)\n",
    "    real_input_mat = torch.tensor(real_input_mat)\n",
    "    real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "    input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "    for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    latents = latents.to(dtype=dtype)\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "# Display\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image).convert(\"L\")\n",
    "              for image in images]  # convert(\"L\")->to gray scale\n",
    "image = pil_images[0]\n",
    "image = image.convert(\"L\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../saved_eta_and_lv_data/json/lvs_matrix_100k.json\"\n",
    "\n",
    "# Load the prompt vectors from the JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    prompts_data = json.load(file)\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 15\n",
    "\n",
    "selected_prompts = [prompts_data[str(i)] for i in range(batch_size)]\n",
    "prompts = [torch.tensor(convert_dim(prompt_vector)).to(\n",
    "    torch_device, dtype=dtype) for prompt_vector in selected_prompts]\n",
    "\n",
    "# Stack processed prompts into a batch\n",
    "input_mat = torch.stack(prompts, dim=0)\n",
    "\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "    for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t,\n",
    "                              encoder_hidden_states=input_mat).sample\n",
    "\n",
    "        # not doing classifier free here!\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    latents = latents.to(dtype=dtype)\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "# Display\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "image = pil_images[0]\n",
    "for i in range(batch_size):\n",
    "    display(pil_images[i].convert(\"L\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    # Path to the directories\n",
    "    image_dir = \"../data/data_resized/bm3d_496_512_test\"\n",
    "    json_dir = \"../data/generate_images/generate_json_latent_test\"\n",
    "\n",
    "    # List all JPEG files in the directory\n",
    "    jpeg_files = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
    "    # Randomly select one JPEG file\n",
    "    selected_jpeg = random.choice(jpeg_files)\n",
    "    # Identify the corresponding JSON file\n",
    "    json_filename = selected_jpeg.replace('.jpeg', '.json')\n",
    "    json_path = os.path.join(json_dir, json_filename)\n",
    "\n",
    "    # Read the vector from the JSON file\n",
    "    with open(json_path, 'r') as file:\n",
    "        prompt = json.load(file)  # Assuming the JSON structure directly contains the vector\n",
    "\n",
    "    # Load and display the original image\n",
    "    original_image_path = os.path.join(image_dir, selected_jpeg)\n",
    "    original_image = Image.open(original_image_path)\n",
    "\n",
    "\n",
    "    height = 512\n",
    "    width = 512\n",
    "    num_inference_steps = 50\n",
    "    seed_nr = np.random.randint(10000)\n",
    "    print(f\"seed_nr {seed_nr}\")\n",
    "    generator = torch.manual_seed(seed_nr)\n",
    "    batch_size = 1\n",
    "\n",
    "    def set_timesteps(scheduler, num_inference_steps):\n",
    "        scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "    # Prep latents\n",
    "    latents = torch.randn(\n",
    "        (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    ).to(torch_device)\n",
    "    latents = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "    # Loop\n",
    "    with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "        real_input = prompt\n",
    "        real_input_mat = convert_dim(real_input)\n",
    "        real_input_mat = torch.tensor(real_input_mat)\n",
    "        real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "        input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "        for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "            latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample  #TODO not doing classifier free here!\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "    with torch.no_grad():\n",
    "        latents = latents.to(dtype=dtype)\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    # Display\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    image = pil_images[0]\n",
    "    image = image.convert(\"L\")\n",
    "    # concatenate the images\n",
    "    concatenated = Image.new('L', (2 * width, height))\n",
    "    # resize original image to with and height\n",
    "    original_image = original_image.resize((width, height))\n",
    "    concatenated.paste(original_image, (0, 0))\n",
    "    concatenated.paste(image, (width, 0))\n",
    "    # display(concatenated)\n",
    "    save_dir = \"/store/CIA/scfc3/diffusion/TemporalRetinaVAE/data/generate_images/diffusion_compare_rel_synth\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Find the highest numbered file in the directory\n",
    "    existing_files = [f for f in os.listdir(save_dir) if f.endswith('.jpeg')]\n",
    "    # Filter out files that do not start with a digit before finding the highest number\n",
    "    existing_files_filtered = [f for f in existing_files if f.split('.')[0].isdigit()]\n",
    "    if existing_files_filtered:\n",
    "        highest_num = max([int(f.split('.')[0]) for f in existing_files_filtered])\n",
    "    else:\n",
    "        highest_num = 0\n",
    "\n",
    "    # File name for the new concatenated image\n",
    "    new_file_name = f\"{highest_num + 1}.jpeg\"\n",
    "    save_path = os.path.join(save_dir, new_file_name)\n",
    "\n",
    "    # Save the concatenated image\n",
    "    concatenated.save(save_path)\n",
    "    print(f\"Concatenated image saved as: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between two prompts\n",
    "prompt_1 = [-0.010155955,-0.004592131,-0.38701367,-0.022426253,-0.00024568333,0.03570405,-0.025474727,-0.003766098,0.00015272689,-0.009874655,0.008434392,-0.012125211,-0.0005047297,0.03926266,-0.005911234,0.012074554,0.025061749,0.016118947,-0.025295403,0.0043429234,-0.019303171,-0.008965155,0.0048098783,-0.022155657,-0.018439487,0.012438098,-0.008230246,-0.011369854,0.013975238,-0.024733193,-0.0024420747,-0.007300651,-0.010927623,0.009804951,0.013800517,-0.010402694,-0.0024602013,0.010395709,-0.026068788,0.007702348,0.014202778,0.0013273212,0.010732115,0.011755251,-0.0026727943,-0.00014252868,-0.00964251,-0.01090727,-0.0086603165,0.91866994,0.0036692163,0.0012229425,0.012757576,-0.009995481,-0.019993355,0.01013756,0.024875883,0.008574486,-0.000510592,-0.013572663,0.0047257543,0.033691775,-0.004811735,-0.0038559795,-0.023267845,0.005499406,-0.19547385,0.021643838,0.007663427,0.01097489,0.02083284,0.0013352409,0.01342792,0.012294747,0.008467486,-0.014881456,0.0070831156,0.004481094,0.030578267,0.010763423,0.022135999,0.004610107,0.0061101215,-0.023164,-0.02054793,-0.01389838,0.012618901,0.0015867227,-0.0007768469,-0.025492975,-2.8981638,1.124453,0.0047108997,-0.0034027295,-0.012984973,-0.0029615848,0.015046431,0.0046694623,-0.009886998,-0.01942618,-0.009058119,0.014815218,0.0002879626,-0.0055417474,-0.0027186424,0.0059638117,0.0034492463,0.010449398,0.021700718,-0.0009963056,0.5486783,-0.014873964,-0.005197687,-0.00982392,0.015218205,0.0066417474,0.007279752,-0.0034043728,0.009700814,-0.014037486,-0.0048284503,-0.009108011,-0.0073809493,0.009162379,-0.024887985,0.003875365,0.0010573778,0.021624224]\n",
    "prompt_2 = [-0.026637983,-0.0049919384,-0.72735846,0.0013894774,1.663604e-6,-0.0037689628,-0.0007240372,0.011690616,0.011068916,0.035094958,0.0075723547,-0.020591881,0.014244843,0.015679605,0.0059791747,0.023565397,-2.31124e-5,0.007694515,-0.010229966,0.010043709,0.011308225,0.01125709,-0.00085738534,-0.0023815772,-0.0067716558,0.013232091,-0.020934608,0.0052047134,0.0031938986,-0.004571068,0.019595314,-0.014215741,0.017283715,0.0058534164,0.013672844,-0.008371335,-0.0011384739,-0.005467668,0.0069078547,-0.022255898,0.023867454,-0.008623882,-0.011428493,0.016150393,-0.010343215,0.009841408,0.0035812396,-0.0038042641,0.00046871044,-0.34486362,-0.04114393,-0.014105927,-0.006903317,0.005046356,0.008912971,0.024923684,-0.03211934,0.008048945,-0.0031226752,0.021200834,0.027791256,-0.012126264,-0.013014689,-0.010026319,-0.011552734,0.0023576792,0.6625984,-0.0037716748,-0.017448092,-0.003082925,0.0055787405,0.013155896,0.012592762,0.013704545,0.014140189,-0.034019794,0.008365242,0.0046707354,0.0045412,0.012962765,0.008882722,-0.0053416304,0.018359562,-0.013084315,0.00395564,-0.013893956,0.019868493,0.009645246,0.0083075445,0.01042995,3.8250873,-0.53500366,-0.0055012787,-0.00024795998,-0.025108527,0.0001758025,0.016005095,-0.014115073,-0.022973824,0.0007388084,-0.009637654,-0.0043257056,0.01392425,-0.007446825,-0.008244691,-0.01408944,-6.8385154e-5,-0.021611854,0.0012386662,-0.0153120095,1.1141413,-0.016341472,-0.0043792315,0.011762388,0.012778547,-0.021212585,-0.038239643,0.013913666,0.027715262,0.013375203,-0.00016425946,-0.04146558,0.00089889695,0.0057990993,0.010341369,0.02737344,-0.00036986242,0.014797182]\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents_orig = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "num_images = 5\n",
    "for k in range(num_images):\n",
    "    latents = latents_orig.clone()\n",
    "\n",
    "    scale = k / (num_images - 1)\n",
    "    prompt = (1-scale) * np.array(prompt_1) + scale * np.array(prompt_2)\n",
    "    # Loop\n",
    "    with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "        real_input = prompt\n",
    "        real_input_mat = convert_dim(real_input)\n",
    "        real_input_mat = torch.tensor(real_input_mat)\n",
    "        real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "        input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "        for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "            latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / vae.config.scaling_factor * latents\n",
    "    with torch.no_grad():\n",
    "        latents = latents.to(dtype=dtype)\n",
    "        image = vae.decode(latents).sample\n",
    "\n",
    "    # Display\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    image = pil_images[0]\n",
    "    image = image.convert(\"L\")\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image grid\n",
    "x = 0\n",
    "base_dir = f\"../data/generate_images/diffusion_images/{x}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "selected_features = np.array(   [92,111,50,3,91,67,37,8,90,120,54,56,21,61,75,29,80,12,95,118,73,94,101,20,48,99,104,13,59,52,106,79,4,86,93,85,72,32,87,35,47,113,40,53,36,55,122,22,5,2,88,77,26,15,7,108,58,28,39,128,126,25,103,65,105,34,18,69,27,43,64,123,38,78,17,121,42,49,33,66,57,6,24,112,10,115,68,45,11,51,41,97,70,102,114,89,71,44,110,109,62,31,124,16,1,74,9,119,14,83,117,76,60,46,23,84,98,82,100,107,81,125,127,30,19,96,63,116]\n",
    ")\n",
    "# subtract 1 from all elements in selected_features (julia -> python)\n",
    "selected_features -= 1\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents_orig = latents * scheduler.init_noise_sigma\n",
    "\n",
    "save_names = [\"verticle_position\", \"tilt\", \"luminance\", \"curvature\", \"morphing\"]\n",
    "cols = 7\n",
    "rows = 5\n",
    "max_val = 4\n",
    "for j in range(rows):\n",
    "    save_name = save_names[j]\n",
    "    for i in range(cols):\n",
    "        prompt = np.zeros(128)\n",
    "        prompt[selected_features[0]] = 3.0\n",
    "        prompt[selected_features[j + 1]] = -max_val + i * 2 * max_val / (cols -1)\n",
    "        latents = latents_orig.clone()\n",
    "\n",
    "        # Loop\n",
    "        with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "            real_input = prompt\n",
    "            real_input_mat = convert_dim(real_input)\n",
    "            real_input_mat = torch.tensor(real_input_mat)\n",
    "            real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "            input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "            for _, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "                latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "                # predict the noise residual\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "                latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        # scale and decode the image latents with vae\n",
    "        latents = 1 / vae.config.scaling_factor * latents\n",
    "        with torch.no_grad():\n",
    "            latents = latents.to(dtype=dtype)\n",
    "            image = vae.decode(latents).sample\n",
    "\n",
    "        # Display\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        image = pil_images[0]\n",
    "        # to Gray scale\n",
    "        image = image.convert(\"L\")\n",
    "        # display(image)\n",
    "        image_path = f\"{base_dir}/{save_name}_{i}.png\"\n",
    "        image.save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nr = 0\n",
    "base_dir = f\"../data/generate_images/diffusion_images\"\n",
    "\n",
    "# Define the names used for saving the images\n",
    "# save_names = [\"verticle_position\", \"tilt\", \"luminance\", \"curvature\", \"morphing\"]\n",
    "save_names = [\"verticle_position\", \"tilt\", \"luminance\", \"curvature\"]\n",
    "\n",
    "# Initialize a list to hold the loaded images\n",
    "loaded_images = []\n",
    "\n",
    "# Load the images\n",
    "for j, save_name in enumerate(save_names):\n",
    "    for i in range(cols):\n",
    "        image_path = f\"{base_dir}/{img_nr}/{save_name}_{i}.png\"\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                loaded_images.append(img.copy())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "# Assuming all images are of the same size\n",
    "img_width, img_height = loaded_images[0].size\n",
    "\n",
    "# Create a new image with a size to hold all the images in a 4x7 grid\n",
    "grid_image = Image.new(\"L\", (img_width * cols, img_height * len(save_names)))\n",
    "\n",
    "# Paste the images into the grid\n",
    "for j, img in enumerate(loaded_images):\n",
    "    x = j % cols * img_width\n",
    "    y = j // cols * img_height\n",
    "    grid_image.paste(img, (x, y))\n",
    "\n",
    "# Display the concatenated image\n",
    "# display(grid_image)\n",
    "\n",
    "# Optionally, save the grid image\n",
    "grid_image_path = f\"{base_dir}/concatenated_grid_{img_nr}.png\"\n",
    "grid_image.save(grid_image_path)\n",
    "print(f\"Concatenated grid image saved at: {grid_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "42d594a2-cc70-4daf-81eb-005e906118d3"
   },
   "outputs": [],
   "source": [
    "def pil_to_latent(input_im):\n",
    "    # if input image is gray sclae convert it to 3 channels\n",
    "    if input_im.mode != 'RGB':\n",
    "        input_im = input_im.convert('RGB')\n",
    "    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(transforms.ToTensor()(input_im).unsqueeze(\n",
    "            0).to(torch_device, dtype=dtype) * 2 - 1)  # Note scaling\n",
    "    return vae.config.scaling_factor * latent.latent_dist.sample()\n",
    "\n",
    "\n",
    "def latents_to_pil(latents):\n",
    "    # bath of latents -> list of images\n",
    "    latents = (1 / vae.config.scaling_factor) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    # to gray scalse\n",
    "    pil_images = [image.convert(\"L\") for image in pil_images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/data_resized/bm3d_496_512_train\"\n",
    "json_dir = \"../data/generate_images/generate_json_latent\"\n",
    "\n",
    "# List all JPEG files in the directory\n",
    "jpeg_files = [f for f in os.listdir(image_dir) if f.endswith('.jpeg')]\n",
    "# Randomly select one JPEG file\n",
    "selected_jpeg = random.choice(jpeg_files)\n",
    "# Identify the corresponding JSON file\n",
    "json_filename = selected_jpeg.replace('.jpeg', '.json')\n",
    "json_path = os.path.join(json_dir, json_filename)\n",
    "\n",
    "# Read the vector from the JSON file\n",
    "with open(json_path, 'r') as file:\n",
    "    # Assuming the JSON structure directly contains the vector\n",
    "    prompt = json.load(file)\n",
    "\n",
    "# Load and display the original image\n",
    "original_image_path = os.path.join(image_dir, selected_jpeg)\n",
    "input_image = Image.open(original_image_path)\n",
    "# resize to height and width\n",
    "input_image = input_image.resize((width, height))\n",
    "input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "063ddf64-a028-4b2b-b300-8970151d5fce",
    "outputId": "664d04c5-af4e-4173-cfa7-78acacda6e3d"
   },
   "outputs": [],
   "source": [
    "# Encode to the latent space\n",
    "encoded = pil_to_latent(input_image)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "a519d92b-7fd3-4b1b-bc98-fae8cdb8819d",
    "outputId": "41e116fc-4875-446e-f836-7f35f512733d"
   },
   "outputs": [],
   "source": [
    "# Let's visualize the four channels of this latent representation:\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for c in range(4):\n",
    "    axs[c].imshow(encoded[0][c].cpu(), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "1b21fa66-1cad-461e-9d60-99045397fca0"
   },
   "source": [
    "This 4x64x64 tensor captures lots of information about the image, hopefully enough that when we feed it through the decoder we get back something very close to our input image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "923be103-ef7c-4bfa-9015-86e085f636b5",
    "outputId": "8adb472f-5784-4b23-b4c8-2c63af02177e"
   },
   "outputs": [],
   "source": [
    "# Decode this latent representation back into an image\n",
    "decoded = latents_to_pil(encoded)[0]\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "de106057-3fc9-498e-a929-9d93a226b94d"
   },
   "outputs": [],
   "source": [
    "# Setting the number of sampling steps:\n",
    "set_timesteps(scheduler, 50)\n",
    "print(f\"len(scheduler.timesteps) = {len(scheduler.timesteps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ef2ab06a-d544-4b3b-a94d-7b9c56465d63",
    "outputId": "5e2e5914-f491-43cc-c6d5-c1850252934b"
   },
   "outputs": [],
   "source": [
    "# See these in terms of the original 1000 steps used for training:\n",
    "print(scheduler.timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "37a106fc-671d-4782-a675-923b148bafdb",
    "outputId": "fe2cafb0-8c7c-45fe-b1c3-1e7c0b81656b"
   },
   "outputs": [],
   "source": [
    "noise = torch.randn_like(encoded)  # Random noise\n",
    "sampling_step = 35\n",
    "encoded_and_noised = scheduler.add_noise(\n",
    "    encoded, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n",
    "latents_to_pil(encoded_and_noised.float())[0]  # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for c in range(4):\n",
    "    axs[c].imshow(encoded_and_noised[0][c].cpu(), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 50\n",
    "seed_nr = np.random.randint(10000)\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "with open(json_path, 'r') as file:\n",
    "    # Assuming the JSON structure directly contains the vector\n",
    "    prompt = json.load(file)\n",
    "# prompt[selected_features[2]] = 3.0\n",
    "print(\"test\")\n",
    "start_step = 0\n",
    "noise = torch.randn((batch_size, unet.config.in_channels, height //\n",
    "                    8, width // 8), generator=generator).to(torch_device)\n",
    "latents = scheduler.add_noise(encoded, noise, timesteps=torch.tensor([\n",
    "                              scheduler.timesteps[start_step]]))\n",
    "latents = latents.to(torch_device).float()\n",
    "# latents = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "    real_input_mat = convert_dim(prompt)\n",
    "    real_input_mat = torch.tensor(real_input_mat)\n",
    "    real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "    input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "    for k, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "        if k >= start_step:\n",
    "            latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(latent_model_input, t,\n",
    "                                  encoder_hidden_states=input_mat).sample\n",
    "\n",
    "            latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "# scale and decode the image latents with vae\n",
    "latents_scale = 1 / vae.config.scaling_factor * latents\n",
    "with torch.no_grad():\n",
    "    latents_scale = latents_scale.to(dtype=dtype)\n",
    "    image = vae.decode(latents_scale).sample\n",
    "    # to gray scale\n",
    "\n",
    "# Display\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "image = pil_images[0]\n",
    "# to gray scale\n",
    "image = image.convert(\"L\")\n",
    "# display(image)\n",
    "# concat image with input_image and display\n",
    "concatenated = Image.new('L', (2 * width, height))\n",
    "input_image = input_image.resize((width, height))\n",
    "concatenated.paste(input_image, (0, 0))\n",
    "concatenated.paste(image, (width, 0))\n",
    "display(concatenated)\n",
    "# take the mse loss IN the latent space of the original and the generated image. print a new decoded image that is the difference between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the mean and the std of the latents and the encoded latents\n",
    "print(f\"mean latents: {latents.mean()}, std latents: {latents.std()}\")\n",
    "print(f\"mean encoded: {encoded.mean()}, std encoded: {encoded.std()}\")\n",
    "\n",
    "# Ensure both latent representations are on the same device and dtype\n",
    "original_latents = encoded.to(torch_device, dtype=dtype)\n",
    "generated_latents = latents.to(torch_device, dtype=dtype)\n",
    "\n",
    "# 1. Compute the MSE in the Latent Space\n",
    "mse_loss = F.mse_loss(original_latents, generated_latents, reduction='mean')\n",
    "print(f\"MSE in Latent Space: {mse_loss.item()}\")\n",
    "\n",
    "# 2. Create a New Image from the Difference in the Latent Space\n",
    "# Here, the new latent representation is the absolute difference between the original and generated latents\n",
    "new_latent_representation = torch.abs(original_latents - generated_latents)\n",
    "\n",
    "# 3. Decode and Display the New Image\n",
    "# Assuming `latents_to_pil` is a function that can decode a latent representation back to an image\n",
    "# and `vae` is your VAE model used for decoding\n",
    "with torch.no_grad():\n",
    "    new_latent_representation = 1 / vae.config.scaling_factor * new_latent_representation\n",
    "    # Adjust scaling if necessary, depending on your VAE model's requirements\n",
    "    new_latent_representation = new_latent_representation.to(dtype=dtype)\n",
    "    # Adjust this line based on your VAE's decode method\n",
    "    new_image_tensor = vae.decode(new_latent_representation).sample\n",
    "\n",
    "# Convert tensor to PIL Image for display\n",
    "new_image_tensor = (new_image_tensor / 2 + 0.5).clamp(0,\n",
    "                                                      1)  # Normalize if necessary\n",
    "new_image_tensor = new_image_tensor.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "new_images = (new_image_tensor * 255).round().astype(\"uint8\")\n",
    "new_pil_images = [Image.fromarray(img) for img in new_images]\n",
    "new_image = new_pil_images[0]\n",
    "# to gray scale\n",
    "# new_image = new_image.convert(\"L\")\n",
    "\n",
    "# Display the new image\n",
    "display(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_tensor(pil_img):\n",
    "    arr = np.array(pil_img)\n",
    "    return torch.tensor(arr).float() / 255.\n",
    "\n",
    "\n",
    "original_image_tensor = pil_to_tensor(input_image).to(\n",
    "    torch_device, dtype=dtype).unsqueeze(0)\n",
    "generated_image_tensor = pil_to_tensor(image).to(\n",
    "    torch_device, dtype=dtype).unsqueeze(0)\n",
    "\n",
    "# Now, proceed with the MSE computation\n",
    "mse_loss_pixel = F.mse_loss(original_image_tensor,\n",
    "                            generated_image_tensor, reduction='mean')\n",
    "print(f\"MSE in Pixel Space: {mse_loss_pixel.item()}\")\n",
    "\n",
    "pixel_difference = torch.abs(original_image_tensor - generated_image_tensor)\n",
    "\n",
    "# Convert the difference back to a PIL image for display\n",
    "pixel_difference_image = pixel_difference.squeeze().cpu()\n",
    "pixel_difference_image = (pixel_difference_image.numpy()\n",
    "                          * 255).round().astype(\"uint8\")\n",
    "new_diff_image = Image.fromarray(\n",
    "    pixel_difference_image, 'L' if pixel_difference_image.ndim == 2 else 'RGB')\n",
    "\n",
    "# Display the new image\n",
    "display(new_diff_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [-0.026637983,-0.0049919384,-0.72735846,0.0013894774,1.663604e-6,-0.0037689628,-0.0007240372,0.011690616,0.011068916,0.035094958,0.0075723547,-0.020591881,0.014244843,0.015679605,0.0059791747,0.023565397,-2.31124e-5,0.007694515,-0.010229966,0.010043709,0.011308225,0.01125709,-0.00085738534,-0.0023815772,-0.0067716558,0.013232091,-0.020934608,0.0052047134,0.0031938986,-0.004571068,0.019595314,-0.014215741,0.017283715,0.0058534164,0.013672844,-0.008371335,-0.0011384739,-0.005467668,0.0069078547,-0.022255898,0.023867454,-0.008623882,-0.011428493,0.016150393,-0.010343215,0.009841408,0.0035812396,-0.0038042641,0.00046871044,-0.34486362,-0.04114393,-0.014105927,-0.006903317,0.005046356,0.008912971,0.024923684,-0.03211934,0.008048945,-0.0031226752,0.021200834,0.027791256,-0.012126264,-0.013014689,-0.010026319,-0.011552734,0.0023576792,0.6625984,-0.0037716748,-0.017448092,-0.003082925,0.0055787405,0.013155896,0.012592762,0.013704545,0.014140189,-0.034019794,0.008365242,0.0046707354,0.0045412,0.012962765,0.008882722,-0.0053416304,0.018359562,-0.013084315,0.00395564,-0.013893956,0.019868493,0.009645246,0.0083075445,0.01042995,3.8250873,-0.53500366,-0.0055012787,-0.00024795998,-0.025108527,0.0001758025,0.016005095,-0.014115073,-0.022973824,0.0007388084,-0.009637654,-0.0043257056,0.01392425,-0.007446825,-0.008244691,-0.01408944,-6.8385154e-5,-0.021611854,0.0012386662,-0.0153120095,1.1141413,-0.016341472,-0.0043792315,0.011762388,0.012778547,-0.021212585,-0.038239643,0.013913666,0.027715262,0.013375203,-0.00016425946,-0.04146558,0.00089889695,0.0057990993,0.010341369,0.02737344,-0.00036986242,0.014797182]\n",
    "\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 20 # 200\n",
    "seed_nr = np.random.randint(10000)\n",
    "seed_nr = 9139\n",
    "print(f\"seed_nr {seed_nr}\")\n",
    "generator = torch.manual_seed(seed_nr)\n",
    "batch_size = 1\n",
    "\n",
    "# Make a folder to store results\n",
    "!rm -rf steps/\n",
    "!mkdir -p steps/\n",
    "\n",
    "\n",
    "def set_timesteps(scheduler, num_inference_steps):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# Prep latents\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ").to(torch_device)\n",
    "latents = (latents * scheduler.init_noise_sigma)\n",
    "\n",
    "# Loop\n",
    "with autocast(\"cuda\"):  # will fallback to CPU if no CUDA; no autocast for MPS\n",
    "\n",
    "    real_input = prompt\n",
    "    real_input_mat = convert_dim(real_input)\n",
    "    real_input_mat = torch.tensor(real_input_mat)\n",
    "    real_input_mat = real_input_mat.to(torch_device, dtype=dtype)\n",
    "    input_mat = real_input_mat.unsqueeze(0)\n",
    "\n",
    "    for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "        latent_model_input = scheduler.scale_model_input(latents, t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=input_mat).sample\n",
    "\n",
    "        latents_x0 = scheduler.step(noise_pred, t, latents).pred_original_sample\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "        # # FOR DUAL IMAGES\n",
    "        # # To PIL Images\n",
    "        # im_t0 = latents_to_pil(latents_x0)[0]\n",
    "        # im_next = latents_to_pil(latents)[0]\n",
    "        # # Combine the two images and save for later viewing\n",
    "        # im = Image.new('RGB', (1024, 512))\n",
    "        # im.paste(im_next, (0, 0))\n",
    "        # im.paste(im_t0, (512, 0))\n",
    "        # im.save(f'steps/{i:04}.png')\n",
    "\n",
    "\n",
    "        im_t0 = latents_to_pil(latents)[0]\n",
    "        # im_t0 = latents_to_pil(latents_x0)[0]\n",
    "        im = Image.new('RGB', (512, 512))\n",
    "        im.paste(im_t0, (0, 0))\n",
    "        im.save(f'steps/{i:04}.png')\n",
    "\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import imageio\n",
    "# import imageio.v2 as imageio\n",
    "\n",
    "# import os\n",
    "\n",
    "# # Define the directory where your images are stored\n",
    "# image_directory = 'steps'\n",
    "# image_files = [os.path.join(image_directory, img) for img in sorted(os.listdir(image_directory)) if img.endswith('.png')]\n",
    "\n",
    "# # Define the output video file name\n",
    "# output_video_file = 'out.mp4'\n",
    "\n",
    "# # Create a writer object specifying the output file and framerate\n",
    "# writer = imageio.get_writer(output_video_file, fps=20, codec='libx264', quality=10, pixelformat='yuv420p')\n",
    "\n",
    "# # Iterate over image files, add them to the video\n",
    "# for image_file in image_files:\n",
    "#     image = imageio.imread(image_file)\n",
    "#     writer.append_data(image)\n",
    "\n",
    "# # Close the writer to finalize the video\n",
    "# writer.close()\n",
    "\n",
    "# # Display the video in a Jupyter notebook\n",
    "# from IPython.display import Video\n",
    "\n",
    "# Video(output_video_file, embed=True, width=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "import os\n",
    "\n",
    "# Define the directory where your images are stored\n",
    "image_directory = 'steps'\n",
    "image_files = [os.path.join(image_directory, img) for img in sorted(os.listdir(image_directory)) if img.endswith('.png')]\n",
    "\n",
    "# Define the output video file name\n",
    "output_video_file = 'out.mp4'\n",
    "\n",
    "# Create a writer object specifying the output file and framerate\n",
    "writer = imageio.get_writer(output_video_file)\n",
    "\n",
    "# Iterate over image files, add them to the video\n",
    "for image_file in image_files:\n",
    "    image = imageio.imread(image_file)\n",
    "    writer.append_data(image)\n",
    "\n",
    "# Close the writer to finalize the video\n",
    "writer.close()\n",
    "\n",
    "# Display the video in a Jupyter notebook\n",
    "from IPython.display import Video\n",
    "\n",
    "Video(output_video_file, embed=True, width=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define the directory where your images are stored\n",
    "image_directory = 'steps'\n",
    "image_files = [os.path.join(image_directory, img) for img in sorted(os.listdir(image_directory)) if img.endswith('.png')]\n",
    "\n",
    "# Define the output video file name\n",
    "output_video_file = 'out.mp4'\n",
    "\n",
    "# Assume the first image to get the size\n",
    "frame = cv2.imread(image_files[0])\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "# Create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec used to create the video\n",
    "video = cv2.VideoWriter(output_video_file, fourcc, 20, (width, height))\n",
    "\n",
    "# Iterate over image files, add them to the video\n",
    "for image_file in image_files:\n",
    "    img = cv2.imread(image_file)\n",
    "    video.write(img)\n",
    "\n",
    "# Close the video writer\n",
    "video.release()\n",
    "\n",
    "# To display the video in a Jupyter notebook, you can still use IPython display\n",
    "from IPython.display import Video\n",
    "Video(output_video_file, embed=True, width=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "07c982ae-2286-48f0-b4e9-7a7651db56ec"
   },
   "source": [
    "# Guidance\n",
    "\n",
    "\n",
    "Extra control to this generation process?\n",
    "\n",
    "At each step, we're going to use our model as before to predict the noise component of x. Then we'll use this to produce a predicted output image, and apply some loss function to this image.\n",
    "\n",
    "This function can be anything, but let's demo with a super simple example. If we want images that have a lot of blue, we can craft a loss function that gives a high loss if pixels have a low blue component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "b20a3505-83b3-44f7-8943-f8bd57c74375"
   },
   "outputs": [],
   "source": [
    "def blue_loss(images):\n",
    "    # How far are the blue channel values to 0.9:\n",
    "    error = torch.abs(images[:, 2] - 0.9).mean()  # [:,2] -> all images in batch, only the blue channel\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "b5ff7db4-c791-42b5-824e-d17c4fef0b67"
   },
   "source": [
    "During each update step, we find the gradient of the loss with respect to the current noisy latents, and tweak them in the direction that reduces this loss as well as performing the normal update step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705,
     "referenced_widgets": [
      "eeb86d5fb2e545b8959ee90bd1c62a20",
      "b1e366a9699d421998d1d46950d26c42",
      "0fcc21c768a245a28569f02842c7be66",
      "2676cd8fb3b94acbb84d0b3f3410ccf4",
      "ad2302e95cbf4104a197c047f5fb3fc1",
      "dcc67ef8d925408cb477f70b76f8beab",
      "2e5f609447144fd6a510893f34fbae61",
      "869af4d345d2466c8c08688a63edcd16",
      "bc53b484b790487e8002cca9314164f8",
      "3bcc77a22c1c43019b932064cb0569cc",
      "80e53210cd9c492d868718fd74dbed3f"
     ]
    },
    "id": "7c8020ed-36cb-42e9-a108-8b53ef247f4c",
    "outputId": "da541ef9-1a3e-4af6-a99d-d695b70481c1"
   },
   "outputs": [],
   "source": [
    "# prompt = \"A campfire (oil on canvas)\"  # @param\n",
    "# height = 512  # default height of Stable Diffusion\n",
    "# width = 512  # default width of Stable Diffusion\n",
    "# num_inference_steps = 50  # @param           # Number of denoising steps\n",
    "# guidance_scale = 8  # @param               # Scale for classifier-free guidance\n",
    "# generator = torch.manual_seed(32)  # Seed generator to create the inital latent noise\n",
    "# batch_size = 1\n",
    "# blue_loss_scale = 200  # @param\n",
    "\n",
    "# # Prep text\n",
    "# text_input = tokenizer(\n",
    "#     [prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "# )\n",
    "# with torch.no_grad():\n",
    "#     text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "# # And the uncond. input as before:\n",
    "# max_length = text_input.input_ids.shape[-1]\n",
    "# uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "# text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "# # Prep Scheduler\n",
    "# set_timesteps(scheduler, num_inference_steps)\n",
    "\n",
    "# # Prep latents\n",
    "# latents = torch.randn(\n",
    "#     (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "#     generator=generator,\n",
    "# )\n",
    "# latents = latents.to(torch_device)\n",
    "# latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "# # Loop\n",
    "# for i, t in tqdm(enumerate(scheduler.timesteps), total=len(scheduler.timesteps)):\n",
    "#     # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "#     latent_model_input = torch.cat([latents] * 2)\n",
    "#     latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "#     # predict the noise residual\n",
    "#     with torch.no_grad():\n",
    "#         noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
    "\n",
    "#     # perform CFG\n",
    "#     noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "#     noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "#     #### ADDITIONAL GUIDANCE ###\n",
    "#     if i % 5 == 0:\n",
    "#         # Requires grad on the latents\n",
    "#         latents = latents.detach().requires_grad_()\n",
    "\n",
    "#         # Get the predicted x0:\n",
    "#         latents_x0 = scheduler.step(noise_pred, t, latents).pred_original_sample\n",
    "\n",
    "#         # Decode to image space\n",
    "#         denoised_images = vae.decode((1 / vae.config.scaling_factor) * latents_x0).sample / 2 + 0.5  # range (0, 1)\n",
    "\n",
    "#         # Calculate loss\n",
    "#         loss = blue_loss(denoised_images) * blue_loss_scale\n",
    "\n",
    "#         # Occasionally print it out\n",
    "#         if i % 10 == 0:\n",
    "#             print(i, \"loss:\", loss.item())\n",
    "\n",
    "#         # Get gradient\n",
    "#         cond_grad = torch.autograd.grad(loss, latents)[0]\n",
    "\n",
    "#         # Modify the latents based on this gradient\n",
    "#         # latents = latents.detach() - cond_grad * sigma**2 # TODO maybe need to replace this line with the right code...\n",
    "\n",
    "#     # Now step with scheduler\n",
    "#     latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "\n",
    "# latents_to_pil(latents)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "32256c70-3839-4c4f-8a48-aaeeee312e79"
   },
   "source": [
    "Tweak the scale (`blue_loss_scale`) - at low values, the image is mostly red and orange thanks to the prompt. At higher values, it is mostly bluish! Too high and we get a plain blue image.\n",
    "\n",
    "Since this is slow, you'll notice that I only apply this loss once every 5 iterations - this was a suggestion from Jeremy and we left it in because for this demo it saves time and still works. For your own tests you may want to explore using a lower scale for the loss and applying it every iteration instead :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "624ea2e9-6e55-491c-8ae8-cb8cebc68800"
   },
   "source": [
    "NB: We should set latents requires_grad=True **before** we do the forward pass of the unet (removing `with torch.no_grad()`) if we want mode accurate gradients. BUT this requires a lot of extra memory. You'll see both approaches used depending on whose implementation you're looking at.\n",
    "\n",
    "Guiding with classifier models can give you images of a specific class. Guiding with a model like CLIP can help better match a text prompt. Guiding with a style loss can help add a particular style. Guiding with some sort of perceptual loss can force it towards the overall look af a target image. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# change wd to /home/scfc3/rds/rds-cbs31-cmih-covid19/user_files/scfc3_files/diffusion/bloodimage\n",
    "os.chdir(\"/home/scfc3/rds/rds-cbs31-cmih-covid19/user_files/scfc3_files/diffusion/bloodimage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "fd788f2d"
   },
   "outputs": [],
   "source": [
    "!sh /home/scfc3/rds/rds-cbs31-cmih-covid19/user_files/scfc3_files/diffusion/bloodimage/train_script_small.sh"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "diff_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
